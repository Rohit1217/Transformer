{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "MttdSC3QU5cJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jpkTv0I6GPws"
      },
      "outputs": [],
      "source": [
        "import torch, torchvision, torchaudio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "att=torch.tril(torch.ones(3,3).view(1,1,3,3))\n",
        "print(att)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoYRMIAMzgeG",
        "outputId": "acb82626-39e2-4833-ba43-8e1a9a438a6d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1., 0., 0.],\n",
            "          [1., 1., 0.],\n",
            "          [1., 1., 1.]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Positional_encoding(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Positional_encoding, self).__init__()\n",
        "    return\n",
        "\n",
        "  def forward(self,x):\n",
        "    B,T,C=x.size()\n",
        "    pos_emb=torch.tensor([[ math.cos(pos / 10000 ** ((2 * i +1)/ C)) if i % 2 == 1 else math.sin(pos / 10000 ** (2 * i / C)) for i in range(C)] for pos in range(T)])\n",
        "    pos_emb.view(1,T,C)\n",
        "    return pos_emb+x\n",
        "\n",
        "pos_emb=Positional_encoding()\n",
        "array=torch.tensor([[[2,2,3,4,6,1],[1,2,3,4,5,5],[1,2,3,4,5,5]],[[2,2,3,4,1,1],[1,2,3,4,2,2],[1,2,3,4,5,5]]],dtype=torch.float32)\n",
        "pos_emb.forward(array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh5vtGnRV9JA",
        "outputId": "6910b491-16ce-4dac-8010-ab8f37cb15b4"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[2.0000, 3.0000, 3.0000, 5.0000, 6.0000, 2.0000],\n",
              "         [1.8415, 2.9999, 3.0022, 5.0000, 5.0000, 6.0000],\n",
              "         [1.9093, 2.9998, 3.0043, 5.0000, 5.0000, 6.0000]],\n",
              "\n",
              "        [[2.0000, 3.0000, 3.0000, 5.0000, 1.0000, 2.0000],\n",
              "         [1.8415, 2.9999, 3.0022, 5.0000, 2.0000, 3.0000],\n",
              "         [1.9093, 2.9998, 3.0043, 5.0000, 5.0000, 6.0000]]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Self_attention(nn.Module):\n",
        "  def __init__(self,head,emb_dim):\n",
        "    super(Self_attention, self).__init__()\n",
        "    self.dim=int(emb_dim/head)\n",
        "    self.emb_dim=emb_dim\n",
        "    self.head=head\n",
        "    self.w_k=nn.Linear(emb_dim,emb_dim)\n",
        "    self.w_q=nn.Linear(emb_dim,emb_dim)\n",
        "    self.w_v=nn.Linear(emb_dim,emb_dim)\n",
        "    self.w_o=nn.Linear(emb_dim,emb_dim)\n",
        "\n",
        "\n",
        "  def forward(self,q=None,k=None,v=None,x=None,mask=None):\n",
        "    if x!=None:\n",
        "      B,T,C=x.size() #batch_size,sequence_size,encoding_dimension\n",
        "      T=int(T)\n",
        "      q,k,v=self.w_q(x),self.w_k(x),self.w_v(x)\n",
        "      q,k,v=q.view(B,T,self.head,self.dim),k.view(B,T,self.head,self.dim),v.view(B,T,self.head,self.dim)\n",
        "      q,k,v=(q.transpose(1,2)),k.transpose(1,2),v.transpose(1,2) #each head gets  seperate qkv dim -b head T embedding\n",
        "      softmax=nn.Softmax(dim=-1)\n",
        "\n",
        "    else:\n",
        "      B,T,C=v.size()\n",
        "      q,k,v=q,k,self.w_v(v)\n",
        "      q,k,v=q.view(B,T,self.head,self.dim),k.view(B,T,self.head,self.dim),v.view(B,T,self.head,self.dim)\n",
        "      q,k,v=(q.transpose(1,2)),k.transpose(1,2),v.transpose(1,2) #each head gets  seperate qkv dim -b head T embedding\n",
        "      softmax=nn.Softmax(dim=-1)\n",
        "\n",
        "    if mask==None:\n",
        "      attention=softmax(torch.matmul(q,k.transpose(2,3)))\n",
        "      attention=torch.matmul(attention,v)\n",
        "\n",
        "    else:\n",
        "      mask=torch.tril(torch.ones(self.dim,self.dim).view(1,1,self.dim,self.dim))\n",
        "      attention=torch.matmul(q,k.transpose(2,3))\n",
        "      attention=attention.masked_fill(mask[:,:,:,:]==0,-1e6)\n",
        "      #for i in range(T):\n",
        "        #for j in range(i+1,T):\n",
        "          #attention[:,:,i,j]=-1e6\n",
        "      attention=softmax(attention)\n",
        "      attention=torch.matmul(attention,v)\n",
        "\n",
        "    attention=attention.transpose(1,2).contiguous().view(B,T,C)\n",
        "    attention=self.w_o(attention)\n",
        "    return attention\n",
        "\n",
        "array=torch.tensor([[[2,2,3,4,6,1],[1,2,3,4,5,5],[1,2,3,4,5,5]],[[2,2,3,4,1,1],[1,2,3,4,2,2],[1,2,3,4,5,5]]],dtype=torch.float32)\n",
        "a=Self_attention(2,6)\n",
        "a.forward(x=array,mask=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDlA4OzV4gFJ",
        "outputId": "05818335-5696-4933-960a-d704e97aed32"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 1.5500,  0.0532, -1.0138,  1.6964, -0.3638, -1.2509],\n",
              "         [ 1.5474,  0.0544, -1.0094,  1.6921, -0.3564, -1.2314],\n",
              "         [ 1.5449,  0.0564, -1.0060,  1.6894, -0.3516, -1.2174]],\n",
              "\n",
              "        [[ 0.3530, -0.7131, -0.5057,  0.3594, -0.0983, -0.8502],\n",
              "         [ 0.4449, -0.5823, -0.5251,  0.4445, -0.0979, -0.8111],\n",
              "         [ 0.4189, -0.6340, -0.5240,  0.4428, -0.0979, -0.8345]]],\n",
              "       grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer_norm(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Layer_norm,self).__init__()\n",
        "    self.w=nn.parameter.Parameter(torch.ones(1))\n",
        "    self.b=nn.parameter.Parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean=torch.mean(x)\n",
        "    var=torch.var(x)\n",
        "    x=(x-mean)/var\n",
        "    x=self.w*x+self.b\n",
        "    return x\n",
        "\n",
        "\n",
        "layer=Layer_norm()\n",
        "x=torch.tensor([[1,2,3,4,5]],dtype=torch.float32)\n",
        "layer.forward(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pTi0lXI54Pg",
        "outputId": "a30e5342-320a-4bfd-f6e6-70d14205ab72"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.8000, -0.4000,  0.0000,  0.4000,  0.8000]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_block(nn.Module):\n",
        "  def __init__(self,input_dim,h_dim,head):\n",
        "    super(Encoder_block,self).__init__()\n",
        "    self.in_dim=input_dim\n",
        "    self.h_dim=h_dim\n",
        "    self.head=head\n",
        "    self.linear1=nn.Linear(self.in_dim,h_dim)\n",
        "    self.linear2=nn.Linear(self.h_dim,self.in_dim)\n",
        "\n",
        "  def forward(self,x):\n",
        "    self_attention=Self_attention(self.head,self.in_dim)\n",
        "    layer=Layer_norm()\n",
        "    residual=x\n",
        "    x=self_attention(x=x)\n",
        "    x=layer(x+residual)\n",
        "    layer2=Layer_norm()\n",
        "    residual2=x\n",
        "    x=F.gelu(self.linear1(x))\n",
        "    x=self.linear2(x)\n",
        "    x=layer2(x+residual2)\n",
        "    return x"
      ],
      "metadata": {
        "id": "MEwa2XaP6XYy"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e=Encoder_block(6,4,2)\n",
        "array=torch.tensor([[[2,2,3,4,6,1],[1,2,3,4,5,5],[1,2,3,4,5,5]],[[2,2,3,4,1,1],[1,2,3,4,2,2],[1,2,3,4,5,5]]],dtype=torch.float32)\n",
        "e.forward(array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwvj0EGzXxbk",
        "outputId": "56fcfed7-8053-4e2c-fc61-babc81233c1b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.5685, -0.2285,  1.2707,  0.0991,  2.8894, -3.3499],\n",
              "         [-2.3397, -0.0880,  1.2478,  0.3311,  1.9183,  1.0970],\n",
              "         [-2.3397, -0.0880,  1.2478,  0.3311,  1.9183,  1.0970]],\n",
              "\n",
              "        [[-1.4078,  0.5743,  0.8062,  0.4762, -1.6386, -2.5291],\n",
              "         [-2.3696,  0.5848,  0.9089,  0.5564, -0.6804, -1.6592],\n",
              "         [-2.6044,  0.4656,  0.8623,  0.6555,  2.2901,  1.2631]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder_block(nn.Module):\n",
        "  def __init__(self,input_dim,h_dim,head):\n",
        "    super(Decoder_block,self).__init__()\n",
        "    self.in_dim=input_dim\n",
        "    self.head=head\n",
        "    self.h_dim=h_dim\n",
        "    self.linear1=nn.Linear(self.in_dim,h_dim)\n",
        "    self.linear2=nn.Linear(self.h_dim,self.in_dim)\n",
        "\n",
        "  def forward(self,q,k,x):\n",
        "    masked_self_attention=Self_attention(self.head,self.in_dim)\n",
        "    layer=Layer_norm()\n",
        "    residual=x\n",
        "    x=masked_self_attention(x=x,mask=True)\n",
        "    x=layer(x+residual)\n",
        "\n",
        "\n",
        "    layer2=Layer_norm()\n",
        "    self_attention=Self_attention(self.head,self.in_dim)\n",
        "    residual=x\n",
        "    x=self_attention(q=q,k=k,v=x)\n",
        "    x=layer2(x+residual)\n",
        "\n",
        "    residual=x\n",
        "    layer3=Layer_norm()\n",
        "    x=F.gelu(self.linear1(x))\n",
        "    x=self.linear2(x)\n",
        "    x=layer3(x+residual)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "qJOkSD5h6sFW"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e=Decoder_block(6,4,2)\n",
        "array=torch.tensor([[[2,2,3,4,6,1],[1,2,3,4,5,5],[1,2,3,4,5,5]],[[2,2,3,4,1,1],[1,2,3,4,2,2],[1,2,3,4,5,5]]],dtype=torch.float32)\n",
        "e.forward(array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8HBSv28Y6U_",
        "outputId": "5e0dc84b-5f1a-410b-f2b6-0b1658d3efb3"
      },
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.0371, -0.0079,  1.3871,  0.3024,  0.3389, -1.6315],\n",
              "         [-0.1295, -0.1835,  1.1840,  0.3577,  0.0104, -0.1949],\n",
              "         [-0.1291, -0.1887,  1.1821,  0.3527,  0.0105, -0.1986]],\n",
              "\n",
              "        [[-0.0432, -0.1530,  1.1782,  0.3108, -1.6232, -1.7156],\n",
              "         [-0.4390, -0.1456,  1.1547,  0.3261, -1.1648, -1.3221],\n",
              "         [-0.3177, -0.1530,  0.9937,  0.4494,  0.2062, -0.0410]]],\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self,input_dim,n,head,h_dim):\n",
        "    super(Transformer,self).__init__()\n",
        "    self.n=n\n",
        "    self.h_dim=h_dim\n",
        "    self.head=head\n",
        "    self.in_dim=input_dim\n",
        "    self.encoder_list=[ Encoder_block(self.in_dim,self.h_dim,self.head) for i in range(n)]\n",
        "    self.decoder_list=[ Decoder_block(self.in_dim,self.h_dim,self.head) for i in range(n)]\n",
        "    self.w_q,self.w_k=nn.Linear(self.in_dim,self.in_dim),nn.Linear(self.in_dim,self.in_dim)\n",
        "    self.linear1=nn.Linear(self.in_dim,h_dim)\n",
        "    self.linear2=nn.Linear(self.h_dim,self.in_dim)\n",
        "\n",
        "  def forward(self,x,y):\n",
        "    pos1,pos2=Positional_encoding(),Positional_encoding()\n",
        "    x,y=pos1(x),pos2(y)\n",
        "    for encoder in self.encoder_list:\n",
        "      x=encoder(x)\n",
        "    q,k=self.w_q(x),self.w_k(x)\n",
        "    x=y\n",
        "    for decoder in self.decoder_list:\n",
        "      x=decoder(q,k,x)\n",
        "    x=F.gelu(self.linear1(x))\n",
        "    x=F.softmax(self.linear2(x),dim=-1)\n",
        "    return x\n",
        "\n",
        "transformer=Transformer(6,2,2,24)\n",
        "array=torch.tensor([[[2,2,3,4,6,1],[1,2,3,4,5,5],[1,2,3,4,5,5]],[[2,2,3,4,1,1],[1,2,3,4,2,2],[1,2,3,4,5,5]]],dtype=torch.float32)\n",
        "transformer.forward(array,array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7yW1e9B7Ew6",
        "outputId": "113a21bd-ce1d-4f27-cd33-818524c4be7b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.1129, 0.1441, 0.2013, 0.2410, 0.1234, 0.1774],\n",
              "         [0.1010, 0.1104, 0.2203, 0.2697, 0.0649, 0.2337],\n",
              "         [0.0977, 0.1080, 0.2235, 0.2767, 0.0613, 0.2328]],\n",
              "\n",
              "        [[0.0844, 0.1064, 0.2257, 0.3596, 0.0799, 0.1440],\n",
              "         [0.0924, 0.1046, 0.2186, 0.3480, 0.0799, 0.1564],\n",
              "         [0.0939, 0.1064, 0.2274, 0.2645, 0.0587, 0.2490]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "()"
      ],
      "metadata": {
        "id": "vx7hI6S3JhaB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}